Description
The purpose of this project is to develop a Map-Reduce program on Hadoop to multiply two sparse matrices.

This project must be done individually. No copying is permitted. Note: We will use a system for detecting software plagiarism, called MossLinks to an external site., which is an automatic system for determining the similarity of programs. That is, your program will be compared with the programs of the other students in class as well as with the programs submitted in previous years. This program will find similarities even if you rename variables, move code, change code structure, etc.

Note that, if you use a Search Engine to find similar programs on the web, we will find these programs too. So don't do it because you will get caught and you will get an F in the course (this is cheating). Don't look for code to use for your project on the web or from other students (current or past). Don't use ChatGPT or any other AI program to generate code. Just do your project alone using the help given in this project description and from your instructor and GTAs only.

Platform
You will develop your program on SDSC Expanse. Optionally, you may use your laptop/PC to develop your program first and then, after you make sure that it works there, you transfer it and test it to Expanse. You may also use IntelliJ IDEA or Eclipse to help you develop your program, if you have done so in Project 1. Note that it is required that you test your programs on Expanse before you submit them.

Project Description
For this project, you are asked to implement matrix multiplication in Map-Reduce. This is described in the last slide in bigdata-l03.pdf and in Section 2.3.9 (page 38) in Map-Reduce and the New Software Stack. Do not use the method in Section 2.3.10. Here is the pseudo code to multiply two matrices M and N. The first Map-Reduce job finds all possible products Mik*Nkj, for all k, by joining M and N:

// mapper for matrix M
map(key,line) =
  split line into 3 values: i,k,m
  emit(k,new Triple(0,i,m))

// mapper for matrix N
map(key,line) =
  split line into 3 values: k,j,n
  emit(k,new Triple(1,j,n))

reduce(k,values) =
  M_values = all triples (0,i,m) from values
  N_values = all triples (1,j,n) from values
  for (0,i,m) in M_values
     for (1,j,n) in N_values
        emit(new Pair(i,j),m*n)
The second Map-Reduce adds all products for each pair (i,j):

map(pair,value) =
  emit(pair,value)

reduce(pair,values) =
  sum = 0.0
  for v in values
    sum += v
  emit(pair,sum)
You should use two Map-Reduce jobs in the same Java file project2/src/main/java/Multiply.java. You should modify Multiply.java only. In your Java main program, args[0] is the first input matrix M, args[1] is the second input matrix N, args[2] is the directory name to pass the intermediate results from the first Map-Reduce job to the second, and args[3] is the output directory. The file formats for the input and output must be text and the file format of the intermediate results must be sequence file format (binary). There are two small sparse matrices 4*3 and 3*3 in the files M-matrix-small.txt and N-matrix-small.txt for testing in local mode. Their matrix multiplication must return the 4*3 matrix in solution-small.txt. Then, there are two moderate-sized matrices 1000*500 and 500*1000 in the files M-matrix-large.txt and M-matrix-large.txt for testing in distributed mode. The first and last lines of the matrix multiplication of these two matrices must must be similar to those in solution-large.txt.

Setting up your project on your laptop
You can use your laptop to develop your program and then test it and run it on Expanse. Note that testing and running your program on Expanse is required. If you do the project on your laptop, download and untar project2:

wget https://lambda.uta.edu/cse6332/project2.tgz
tar xfz project2.tgz
To compile and run project2 on your laptop:

cd project2
mvn install
rm -rf tmp output
~/hadoop-3.2.2/bin/hadoop jar target/*.jar Multiply M-matrix-small.txt N-matrix-small.txt tmp output
The file output/part-r-00000 will contain the results which must be the same as in solution-small.txt.

Update 02/23/23: You can store the output log of your execution using:

rm -rf tmp output
~/hadoop-3.2.2/bin/hadoop jar target/*.jar Multiply M-matrix-small.txt N-matrix-small.txt tmp output &>multiply.local.out
After you make sure that the project works correctly for small data on your laptop, copy the files to Expanse and test it on Expanse.

Setting up your project on Expanse
This step is required. If you'd like, you can develop this project completely on Expanse. If you have already developed project2 on your laptop, copy project2.tgz from your laptop to Expanse. Otherwise, download project2 using wget https://lambda.uta.edu/cse6332/project2.tgz. Then do:

tar xfz project2.tgz
chmod -R g-wrx,o-wrx project2
You can compile Multiply.java on Expanse using:

run multiply.build
and you can run it in local mode over the two small matrices using:

sbatch multiply.local.run
The result in the directory output must be similar to solution-small.txt. You should modify and run your programs in local mode until you get the correct result.

Update 02/23/23: Like in Project 1, if you get errors when you run your program on Expanse in local mode, you may submit the  multiply.local.out generated on your PC/laptop. 

After you make sure that your program runs correctly in local mode, you run it in distributed mode using:

sbatch multiply.distr.run
This will print the first and last lines of the resulting matrix into the log file. These lines must be similar to those in solution-large.txt.

What to submit
On Expanse, make sure that the following files in your project2 directory exist and are correct:

project2/src/main/java/Multiply.java
project2/multiply.local.out
project2/output/part-r-00000
project2/multiply.distr.out
Archive your project2 directory on Expanse using:

tar cfz project2.tgz project2
On your laptop, copy project2.tgz from Expanse to your laptop:

scp xyz1234@login.expanse.sdsc.edu:project2.tgz ./
Finally, upload your project2.tgz from your laptop using this web page.

