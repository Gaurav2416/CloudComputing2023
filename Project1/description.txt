Description
The purpose of this project is to develop a simple Map-Reduce program on Hadoop that analyzes data from Twitter.

This project must be done individually. No copying is permitted. Note: We will use a system for detecting software plagiarism, called MossLinks to an external site., which is an automatic system for determining the similarity of programs. That is, your program will be compared with the programs of the other students in class as well as with the programs submitted in previous years. This program will find similarities even if you rename variables, move code, change code structure, etc.

Note that, if you use a Search Engine to find similar programs on the web, we will find these programs too. So don't do it because you will get caught and you will get an F in the course (this is cheating). Don't look for code to use for your project on the web or from other students (current or past). Just do your project alone using the help given in this project description and from your instructor and GTA only.

Platform
You will develop your program on your laptop and then on SDSC Expanse. Optionally, you may use IntelliJ IDEA or Eclipse to help you develop your program on your laptop, but you should test your programs on Expanse before you submit them.

How to develop your project on your laptop
You can use your laptop to develop your program and then test it and run it on Expanse. This step is optional but highly recommended because it will save you a lot of time. Note that testing and running your program on Expanse is required.

If you have Mac OS or Linux, make sure you have Java and Maven installed. On Mac, you can install Maven using Homebrew: brew install maven. On Ubuntu Linux, use: apt install maven.

On Windows 10, you need to install Windows Subsystem for Linux (WSL 2)Links to an external site. and then Ubuntu 20.04 or 22.04 LTS. It's OK if you have WSL 1 or an older Ubuntu. Then, open a unix shell (terminal) on WSL2 and do:

sudo apt update
sudo apt upgrade
sudo apt install openjdk-8-jdk maven
To install Hadoop and the project on Mac, Linux, or Windows WSL2, cut&paste and execute on the unix shell:

cd
wget https://archive.apache.org/dist/hadoop/common/hadoop-3.2.2/hadoop-3.2.2.tar.gz
tar xfz hadoop-3.2.2.tar.gz
wget http://lambda.uta.edu/cse6332/project1.tgz
tar xfz project1.tgz
You should also set your JAVA_HOME to point to your java installation. For example, on Windows 10 do:

export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
To test Map-Reduce, go to project1/examples/src/main/java and look at the two Map-Reduce examples Simple.java and Join.java. You can compile both Java files using:

cd
cd project1/examples
mvn install
and you can run Simple in standalone mode using:

~/hadoop-3.2.2/bin/hadoop jar target/*.jar Simple simple.txt output-simple
The file output-simple/part-r-00000 will contain the results.

To compile and run project1:

cd
cd project1
mvn install
rm -rf temp output
~/hadoop-3.2.2/bin/hadoop jar target/*.jar Twitter small-twitter.csv temp output
The file output/part-r-00000 must contain the same results as in file small-solution.txt. (The file temp/part-r-00000 contains the temporary results from the first map-reduce job). After your project works correctly on your laptop (it produces the same results as the solution), copy it to Expanse:

cd
rm project1.tgz
tar cfz project1.tgz project1
scp project1.tgz xyz1234@login.expanse.sdsc.edu:
where xyz1234 is your Expanse username.

Setting up your Project on Expanse
This step is required. You need to follow this step only for the first time running your code in Expanse. Follow the directions on how to login on Expanse at SDSC Expanse. Please email the GTA if you need further help.

First, you need to allow password-less login to local host (without it, you can't run Map-Reduce). Login on Expanse. Then do:

ssh localhost
using your password and exit using control-D. Then do:

ssh-keygen -t rsa
(press enter at each line). Then do:

cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod og-wx ~/.ssh/authorized_keys
Now you should be able to ssh localhost without a password.

Then, edit the file .bashrc (note: it starts with a dot - you can see it using ls -a) using a text editor, such as nano .bashrc, and add the following lines at the end (cut-and-paste):

module load gcc openjdk
module load slurm sdsc
SW=/expanse/lustre/projects/uot187/fegaras
alias run='srun --pty -A uot187 --partition=shared --nodes=1 --ntasks-per-node=1 --mem=2G -t 00:05:00 --wait=0 --export=ALL'
logout and login again to apply the changes.

Run your Project on Expanse
If you have already developed project1 on your laptop, copy project1.tgz from your laptop to Expanse. Otherwise, download project1 from the class web site using wget http://lambda.uta.edu/cse6332/project1.tgz. Then untar it using:

tar xfz project1.tgz
rm project1.tgz
chmod -R g-wrx,o-wrx project1
Go to project1/examples and look at the two Map-Reduce examples src/main/java/Simple.java and src/main/java/Join.java. You can compile both Java files using:

run example.build
and you can run them in standalone mode using:

sbatch example.local.run
The file example.local.out will contain the trace log of the Map-Reduce evaluation while the files output-simple/part-r-00000 output-join/part-r-00000 will contain the results.

You can compile Twitter.java on Expanse using:

run twitter.build
and you can run Twitter.java in standalone mode over a small dataset using:

sbatch twitter.local.run
The results generated by your program will be in the directory output. Your results must be the same as in the file small-solution.txt.

You should develop and run your programs in standalone mode until you get the correct result. After you make sure that your program runs correctly in standalone mode, you run it in distributed mode using:

sbatch twitter.distr.run
This will process the data on the large dataset large-twitter.csv (located in /expanse/lustre/projects/uot187/fegaras/twitter/combined_data_1.txt) and will write the result in the directory output-distr. These results should be similar to the results in the file large-solution.txt. Note that running in distributed mode will use up at least 10 of your SUs. So do this a couple of times only, after you make sure that your program works correctly in standalone mode. You can check your SUs using: expanse-client user

Project Description
In this project, you are asked to implement a simple graph algorithm that needs two Map-Reduce jobs. You will use real data from Twitter from 2010. The dataset represents the follower graph that contains links between tweeting users in the form: user_id, follower_id (where user_id is the id of a user and follower_id is the id of the follower). For example:

12,13
12,14
12,15
16,17
Here, users 13, 14 and 15 are followers of user 12, while user 17 is a follower of user 16. The complete dataset is available on Expanse (file /expanse/lustre/projects/uot187/fegaras/large-twitter.csv) and contains 736,930 users and 36,743,448 links. A subset of this file (which contains the last 10,000 lines of the complete dataset) is available in small-twitter.csv inside project1.

First, for each twitter user, you count the number of users she follows. Then, you group the users by their number of the users they follow and for each group you count how many users belong to this group. That is, the result will have lines such as:

10 30
which says that there are 30 users who follow 10 users. To help you, I am giving you the pseudo code. The first Map-Reduce is:

map ( key, line ):
  read 2 integers from the line into the variables id and follower_id (delimiter is comma ",")
  emit( follower_id, id )

reduce ( follower_id, ids ):
  count = 0
  for n in ids
      count++
  emit( follower_id, count )
That is, the ids in the reducer is the sequence of all users followed by the user with follower_id.
The second Map-Reduce is:

map ( key, line ):
  read 2 integers from the line into the variables follower_id and count (delimiter is tab "\t")
  emit( count, 1 )

reduce ( count, values ):
  sum = 0
  for v in values
      sum += v
  emit( count, sum )
You should write the two Map-Reduce job in the Java file src/main/java/Twitter.java. An empty src/main/java/Twitter.java has been provided, as well as scripts to build and run this code on Expanse. You should modify the Twitter.java only. In your Java main program, args[0] is the twitter file, args[1] is the intermediate directory (output of job1 and input of job2), and args[2] is the output directory. All the input and output file formats must be text formats. Do not use binary formats. See slide 13 in bigdata-l03.pdf to see how to chain two jobs together. It will be easier to do this project if you finish the first map-reduce first and check the results in the temporary directory temp and then finish the second map-reduce.

Optional: Use an IDE to develop your project
If you have a prior good experience with an IDE (IntelliJ IDEA or Eclipse), you may want to develop your program using an IDE and then test it and run it on Expanse. Using an IDE is optional; you shouldn't do this if you haven't used an IDE before.

On IntelliJ IDEA, go to New→Project from Existing Sources, then choose your project1 directory, select Maven, and then Finish. To compile the project, go to Run→Edit Configurations, use + to Add New Configuration, select Maven, give it a name, use Working directory: your project1 directory, Command line: install, then Apply.

On Eclipse, you first need to install m2eLinks to an external site. (Maven on Eclipse), if it's not already installed. Then go to Open File...→Import Project from File System, then choose your project1 directory. To compile your project, right click on the project name at the Package Explorer, select Run As, and then Maven install.